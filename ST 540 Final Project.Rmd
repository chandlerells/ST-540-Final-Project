---
title: "ST 540 Final Project"
author: "Chandler Ellsworth"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(readxl)
library(tidyverse)
library(rjags)
library(caret)
```

## Read in Data
```{r}
#read in data
turtles <- read_excel("Turtles.xlsx") %>% 
   select(`Ending disposition`, Head, Neurologic, Respiratory, CoeolomicBreach, 
          Eye, Abscess, Shell, Extremity, Abscess2, HBC2, Infection2, OtherTrauma2, 
          UnknownFracture2, Abscess3, Infection3, Trauma3)
head(turtles)
```

## Create training and test sets
```{r}
#set the seed for reproducibility
set.seed(123)
#create a vector containing the row indices for the training set
train_indices <- createDataPartition(turtles$`Ending disposition`, p = 0.8, list = FALSE)
#create number of repetitions for each number in 3 fold cross validation
index_count <- round(length(train_indices)/3, 0)
#create the training set
train_data <- turtles[train_indices, ] %>% mutate(index = sample(c(rep(1, index_count),
                                                                   rep(2,index_count),
                                                                   rep(3, index_count))))
#create test set
test_data <- turtles[-train_indices, ]
```

## Perform 3 fold Cross-Validation
```{r}
#create empty lists to posterior probabilities and max classifications for each observation in test set
list1 <- list()
list2 <- list()
list3 <- list()
list5 <- list()
list6 <- list()
list7 <- list()
#create loop for cross validation where each time through the loop, the models are fit on the folds 
#not equal to the current index value and predictions are made on the fold equal to the current index value
for(i in 1:3){
  #store training predictors for model 1
  X_train1_train <- train_data[train_data$index != i, ] %>% 
    select(Head, Neurologic, Respiratory, CoeolomicBreach, Eye, Abscess, Shell, Extremity)
  #store test predictors for model 1
  X_train1_test <- train_data[train_data$index == i, ] %>% 
    select(Head, Neurologic, Respiratory, CoeolomicBreach, Eye, Abscess, Shell, Extremity)
  #store training predictors for model 2
  X_train2_train <- train_data[train_data$index != i, ] %>% 
    select(Abscess2, HBC2, Infection2, OtherTrauma2, UnknownFracture2)
  #store test predictors for model 2
  X_train2_test <- train_data[train_data$index == i, ] %>% 
    select(Abscess2, HBC2, Infection2, OtherTrauma2, UnknownFracture2)
  #store training predictors for model 3
  X_train3_train <- train_data[train_data$index != i, ] %>% 
    select(Abscess3, Infection3, Trauma3)
  #store test predictors for model 3
  X_train3_test <- train_data[train_data$index == i, ] %>% 
    select(Abscess3, Infection3, Trauma3)
  #store proper response data for model fitting and prediction
  Y_train_train <- as.numeric(factor(train_data[train_data$index != i, 1]$`Ending disposition`))
  Y_train_test <- as.numeric(factor(train_data[train_data$index == i, 1]$`Ending disposition`))
  #store appropriate lengths and number of predictors
  N_train <- length(Y_train_train)
  N_test <- length(Y_train_test )
  n_train1 <- length(X_train1_train)
  n_train2 <- length(X_train2_train)
  n_train3 <- length(X_train3_train)
  
  #store data for JAGS
  data1   <- list(Y = Y_train_train, X1 = X_train1_train, X2 = X_train1_test, n1 = N_train, n2 = N_test, n3 = n_train1)
  #define the model
  model_string1 <- textConnection("model{
                                  
     # Likelihood
     for(i in 1:n1) {
       Y[i] ~ dcat(p[i, ])
       denom[i] <- 1 + exp(alpha1 + inprod(X1[i,], beta[])) + exp(alpha2 + inprod(X1[i,], gamma[])) 
       p[i, 1] <- 1 / denom[i]
       p[i, 2] <- exp(alpha1 + inprod(X1[i,], beta[])) / denom[i]
       p[i, 3] <- exp(alpha2 + inprod(X1[i,], gamma[])) / denom[i]
     }
     
     # Prediction
     for(i in 1:n2) {
       Y1[i] ~ dcat(p1[i, ])
       denom1[i] <- 1 + exp(alpha1 + inprod(X2[i,], beta[])) + exp(alpha2 + inprod(X2[i,], gamma[])) 
       p1[i, 1] <- 1 / denom1[i]
       p1[i, 2] <- exp(alpha1 + inprod(X2[i,], beta[])) / denom1[i]
       p1[i, 3] <- exp(alpha2 + inprod(X2[i,], gamma[])) / denom1[i]
     }
     
     # Priors
     for(j in 1:n3){
       beta[j] ~ dnorm(0, 100)
       gamma[j] ~ dnorm(0, 100)
     }
     alpha1 ~ dnorm(0, 100)
     alpha2 ~ dnorm(0, 100)
  
  }")
  #load the data and compile the MCMC code
  model1 <- jags.model(model_string1, data = data1, n.chains = 1, quiet = TRUE)
  #burn in for samples
  update(model1, 5000, progress.bar = "none")
  #Generate post-burn-in samples and retain parameters of interest
  samples1 <- coda.samples(model1, variable.names = c("p1", "beta", "gamma", "Y1"), n.iter = 10000, progress.bar = "none")[[1]]
  #calculate PPD means for each of the 3 probabilities for each prediction
  prob_mean1 <- colMeans(samples1[, (N_test+(n_train1*2)+1): (N_test+(n_train1*2)+(N_test*3))])
  #append to list
  list1 <- append(list1, list(prob_mean1))
  #determine the category classified the most frequently for each prediction
  preds1 <- apply(samples1[, 1:N_test], MARGIN = 2, FUN = function(x){
    as.numeric(names(table(x))[which.max(table(x))])})
  #append to list
  list5 <- append(list5, list(preds1))
  
  #store data for JAGS
  data2   <- list(Y = Y_train_train, X1 = X_train2_train, X2 = X_train2_test, n1 = N_train, n2 = N_test, n3 = n_train2)
  #define the model
  model_string2 <- textConnection("model{
                                  
     # Likelihood
     for(i in 1:n1) {
       Y[i] ~ dcat(p[i, ])
       denom[i] <- 1 + exp(alpha1 + inprod(X1[i,], beta[])) + exp(alpha2 + inprod(X1[i,], gamma[])) 
       p[i, 1] <- 1 / denom[i]
       p[i, 2] <- exp(alpha1 + inprod(X1[i,], beta[])) / denom[i]
       p[i, 3] <- exp(alpha2 + inprod(X1[i,], gamma[])) / denom[i]
     }
     
     # Prediction
     for(i in 1:n2) {
       Y1[i] ~ dcat(p1[i, ])
       denom1[i] <- 1 + exp(alpha1 + inprod(X2[i,], beta[])) + exp(alpha2 + inprod(X2[i,], gamma[])) 
       p1[i, 1] <- 1 / denom1[i]
       p1[i, 2] <- exp(alpha1 + inprod(X2[i,], beta[])) / denom1[i]
       p1[i, 3] <- exp(alpha2 + inprod(X2[i,], gamma[])) / denom1[i]
     }
     
     # Priors
     for(j in 1:n3){
       beta[j] ~ dnorm(0, 100)
       gamma[j] ~ dnorm(0, 100)
     }
     alpha1 ~ dnorm(0, 100)
     alpha2 ~ dnorm(0, 100)
  
  }")
  #load the data and compile the MCMC code
  model2 <- jags.model(model_string2, data = data2, n.chains = 1, quiet = TRUE)
  #burn in for samples
  update(model2, 5000, progress.bar = "none")
  #Generate post-burn-in samples and retain parameters of interest
  samples2 <- coda.samples(model2, variable.names = c("p1", "beta", "gamma", "Y1"), n.iter = 10000, progress.bar = "none")[[1]]
  #calculate PPD means for each of the 3 probabilities for each prediction
  prob_mean2 <- colMeans(samples2[, (N_test+(n_train2*2)+1): (N_test+(n_train2*2)+(N_test*3))])
  #append to list
  list2 <- append(list2, list(prob_mean2))
  #determine the category classified the most frequently for each prediction
  preds2 <- apply(samples2[, 1:N_test], MARGIN = 2, FUN = function(x){
    as.numeric(names(table(x))[which.max(table(x))])})
  #append to list
  list6 <- append(list6, list(preds2))
  
  #store data for JAGS
  data3   <- list(Y = Y_train_train, X1 = X_train3_train, X2 = X_train3_test, n1 = N_train, n2 = N_test, n3 = n_train3)
  #define the model  
  model_string3 <- textConnection("model{
                                  
     # Likelihood
     for(i in 1:n1) {
       Y[i] ~ dcat(p[i, ])
       denom[i] <- 1 + exp(alpha1 + inprod(X1[i,], beta[])) + exp(alpha2 + inprod(X1[i,], gamma[])) 
       p[i, 1] <- 1 / denom[i]
       p[i, 2] <- exp(alpha1 + inprod(X1[i,], beta[])) / denom[i]
       p[i, 3] <- exp(alpha2 + inprod(X1[i,], gamma[])) / denom[i]
     }
     
     # Prediction
     for(i in 1:n2) {
       Y1[i] ~ dcat(p1[i, ])
       denom1[i] <- 1 + exp(alpha1 + inprod(X2[i,], beta[])) + exp(alpha2 + inprod(X2[i,], gamma[])) 
       p1[i, 1] <- 1 / denom1[i]
       p1[i, 2] <- exp(alpha1 + inprod(X2[i,], beta[])) / denom1[i]
       p1[i, 3] <- exp(alpha2 + inprod(X2[i,], gamma[])) / denom1[i]
     }
     
     # Priors
     for(j in 1:n3){
       beta[j] ~ dnorm(0, 100)
       gamma[j] ~ dnorm(0, 100)
     }
     alpha1 ~ dnorm(0, 100)
     alpha2 ~ dnorm(0, 100)
  
  }")
  #load the data and compile the MCMC code
  model3 <- jags.model(model_string3, data = data3, n.chains = 1, quiet = TRUE)
  #burn in for samples
  update(model3, 5000, progress.bar = "none")
  #Generate post-burn-in samples and retain parameters of interest
  samples3 <- coda.samples(model3, variable.names = c("p1", "beta", "gamma", "Y1"), n.iter = 10000, progress.bar = "none")[[1]]
  #calculate PPD means for each of the 3 probabilities for each prediction
  prob_mean3 <- colMeans(samples3[, (N_test+(n_train3*2)+1): (N_test+(n_train3*2)+(N_test*3))])
  #append to list
  list3 <- append(list3, list(prob_mean3))
  #determine the category classified the most frequently for each prediction
  preds3 <- apply(samples3[, 1:N_test], MARGIN = 2, FUN = function(x){
    as.numeric(names(table(x))[which.max(table(x))])})
  #append to list
  list7 <- append(list7, list(preds3))
}
```

## Check convergence
```{r}
#check convergence of each model by checking effective sample size of last fold
effectiveSize(samples1[, (N_test + 1):(N_test + (n_train1 * 2))])
effectiveSize(samples2[, (N_test + 1):(N_test + (n_train2 * 2))])
effectiveSize(samples3[, (N_test + 1):(N_test + (n_train3 * 2))])
```
## Determine best model using probabilities
```{r}
#function to split a vector into three equal parts
split_vector <- function(vec) {
  n <- length(vec)
  n_per_col <- ceiling(n / 3)
  as.data.frame(matrix(vec, ncol = 3))
}
#apply the function to each element of the lists
df_list1 <- lapply(list1, split_vector)
df_list2 <- lapply(list2, split_vector)
df_list3 <- lapply(list3, split_vector)
#combine the data frames by column
combined_df1 <- do.call(cbind, df_list1)
combined_df2 <- do.call(cbind, df_list2)
combined_df3 <- do.call(cbind, df_list3)
#create appropriate names for each data frame
names(combined_df1) <- c("Mod1_p1", "Mod1_p2", "Mod1_p3", "Mod2_p1", "Mod2_p2", "Mod2_p3", "Mod3_p1", "Mod3_p2", "Mod3_p3")
names(combined_df2) <- c("Mod1_p1", "Mod1_p2", "Mod1_p3", "Mod2_p1", "Mod2_p2", "Mod2_p3", "Mod3_p1", "Mod3_p2", "Mod3_p3")
names(combined_df3) <- c("Mod1_p1", "Mod1_p2", "Mod1_p3", "Mod2_p1", "Mod2_p2", "Mod2_p3", "Mod3_p1", "Mod3_p2", "Mod3_p3")
#store all probabilities from each model for each prediction
preds <- rbind(combined_df1, combined_df2, combined_df3)
#create proper order of response to compare to each model as sample was used to randomize the index for cv
ordered <- train_data %>% arrange(index) %>% select(`Ending disposition`) %>% unlist() %>% as.factor() %>% as.numeric()
#add response appropriately ordered to preds data frame
preds$Outcome <-ordered %>% unlist() %>% as.numeric()
#in order to determine which model performed the best, only retain probabilities from each model associated with correct category
values <- apply(preds, MARGIN = 1, FUN = function(x){
  if (x[10] == 1) {
    list(mod1 = x[1], mod2 = x[4], mod3 = x[7])
  } else if (x[10] == 2) {
    list(mod1 = x[2], mod2 = x[5], mod3 = x[8])
  } else {
    list(mod1 = x[3], mod2 = x[6], mod3 = x[9])
  }
})
#bind the rows so that there's only columns corresponding to each model and the outcome
filtered_preds <- bind_rows(values) %>% mutate(Outcome = preds$Outcome)
#create new column that says which model had the highest probability corresponding to correct outcome
filtered_preds <-  filtered_preds %>% mutate(best = colnames(filtered_preds)[max.col(filtered_preds[, 1:3], "first")])
#determine which model performed the best by looking at proportion of times each model was closest to correct outcome
round(table(filtered_preds$best)/sum(table(filtered_preds$best)), 3)
round(filtered_preds, 4)
```
## Determine performance of best model
```{r}
#combine predictions from each fold in list into tibble
mod1_pred <- bind_cols(list5)
#create appropriate names for each column
names(mod1_pred) <- c("one", "two", "three")
#create vector of all prediction values
mod1_pred <- c(mod1_pred$one, mod1_pred$two, mod1_pred$three)
#combine predictions from each fold in list into tibble
mod2_pred <- bind_cols(list6)
#create appropriate names for each column
names(mod2_pred) <- c("one", "two", "three")
#create vector of all prediction values
mod2_pred <- c(mod2_pred$one, mod2_pred$two, mod2_pred$three)
#combine predictions from each fold in list into tibble
mod3_pred <- bind_cols(list7)
#create appropriate names for each column
names(mod3_pred) <- c("one", "two", "three")
#create vector of all prediction values
mod3_pred <- c(mod3_pred$one, mod3_pred$two, mod3_pred$three)
#create data frame of response and predicted values from each model
preds <- data.frame(Outcome = ordered %>% unlist() %>% as.numeric(), 
                    mod1_pred, 
                    mod2_pred, 
                    mod3_pred)
#see how models compares to benchmark that classifies everything as the majority class
majority_class <- names(table(train_data$`Ending disposition`))[which.max(table(train_data$`Ending disposition`))]
majority <- ifelse(majority_class == "Death", 1, ifelse(majority_class == "Released", 3, 2))
#create data frame of accuracy for each model and using the benchmark
data.frame(Accuracy = round(c(mean((preds$Outcome == preds$mod1_pred)), 
                   mean((preds$Outcome == preds$mod2_pred)),
                   mean((preds$Outcome == preds$mod3_pred)),
                   mean((preds$Outcome == rep(majority, length(preds$Outcome))))), 5),
           row.names = c("Model1", "Model2", "Model3", "Benchmark"))
```

## Fit Best Model
```{r}
#store training predictors for model 1
X_train <- train_data %>% 
  select(Head, Neurologic, Respiratory, CoeolomicBreach, Eye, Abscess, Shell, Extremity)
#store test predictors for model 1
X_test <- test_data %>% 
  select(Head, Neurologic, Respiratory, CoeolomicBreach, Eye, Abscess, Shell, Extremity)
#store proper response data for model fitting and prediction
Y_train <- as.numeric(factor(train_data$`Ending disposition`))
Y_test <- as.numeric(factor(test_data$`Ending disposition`))
#store appropriate lengths and number of predictors
N_train <- length(Y_train)
N_test <- length(Y_test )
n_train <- length(X_train)

#store data for JAGS
data  <- list(Y = Y_train, X1 = X_train, X2 = X_test, n1 = N_train, n2 = N_test, n3 = n_train)
#define the model
model_string <- textConnection("model{
                                  
  # Likelihood
  for(i in 1:n1) {
    Y[i] ~ dcat(p[i, ])
    denom[i] <- 1 + exp(alpha1 + inprod(X1[i,], beta[])) + exp(alpha2 + inprod(X1[i,], gamma[])) 
    p[i, 1] <- 1 / denom[i]
    p[i, 2] <- exp(alpha1 + inprod(X1[i,], beta[])) / denom[i]
    p[i, 3] <- exp(alpha2 + inprod(X1[i,], gamma[])) / denom[i]
    }
     
  # Prediction
  for(i in 1:n2) {
    Y1[i] ~ dcat(p1[i, ])
    denom1[i] <- 1 + exp(alpha1 + inprod(X2[i,], beta[])) + exp(alpha2 + inprod(X2[i,], gamma[])) 
    p1[i, 1] <- 1 / denom1[i]
    p1[i, 2] <- exp(alpha1 + inprod(X2[i,], beta[])) / denom1[i]
    p1[i, 3] <- exp(alpha2 + inprod(X2[i,], gamma[])) / denom1[i]
    }
     
  # Priors
  for(j in 1:n3){
    beta[j] ~ dnorm(0, 100)
    gamma[j] ~ dnorm(0, 100)
  }
  alpha1 ~ dnorm(0, 100)
  alpha2 ~ dnorm(0, 100)
  
  }")
#load the data and compile the MCMC code
model <- jags.model(model_string, data = data, n.chains = 2, quiet = TRUE)
#burn in for samples
update(model, 5000, progress.bar = "none")
#Generate post-burn-in samples and retain parameters of interest
samples <- coda.samples(model, variable.names = c("p1", "beta", "gamma", "Y1"), n.iter = 10000, progress.bar = "none")
#calculate PPD means for each of the 3 probabilities for each prediction
prob_mean <- colMeans(samples[[1]][, (N_test+(n_train*2)+1): (N_test+(n_train*2)+(N_test*3))])
#determine the category classified the most frequently for each prediction
preds <- apply(samples[[1]][, 1:N_test], MARGIN = 2, FUN = function(x){
    as.numeric(names(table(x))[which.max(table(x))])})
```

```{r}
#check convergence 
effectiveSize(samples[[1]][, (N_test + 1):(N_test + (n_train * 2))])
gelman.diag(samples[, (N_test + 1):(N_test + (n_train * 2))])
#analyze summary
summary(samples[[1]][, (N_test + 1):(N_test + (n_train * 2))])
```

### Determine performance of best model
```{r}
#see how model compares to benchmark that classifies everything as the majority class in test set
majority_class <- names(table(test_data$`Ending disposition`))[which.max(table(test_data$`Ending disposition`))]
majority <- ifelse(majority_class == "Death", 1, ifelse(majority_class == "Released", 3, 2))
#create data frame of accuracy for  model and benchmark
round(data.frame(Accuracy = c(mean(Y_test == preds), mean(Y_test == rep(majority, length(Y_test)))),
           row.names = c("Mod1", "Benchmark")),4)
```


```{r}
#store probabilities for first observation
p1_1 <- samples[[1]][, (N_test + (n_train*2) + 1)]
p1_2 <- samples[[1]][, ((N_test*2) + (n_train*2) + 1)]
p1_3 <- samples[[1]][, ((N_test*3) + (n_train*2) + 1)]

#create plot margins
par(mar = c(4, 4, 0.2, 0.2), cex.axis= 1, cex.lab= 1.2)
# Plot smoothed densities
plot(density(p1_1), col = "blue", xlim = c(0.17,0.6), ylim = c(0, 30),
     main = NA, xlab = "Probability", ylab = "Density")
lines(density(p1_2), col = "red")
lines(density(p1_3), col = "darkgreen")
legend("topright", legend = c("Death", "Euthanized", "Released"),
       col = c("blue", "red", "darkgreen"), lty = 1)
# Add points for peak values and annotate them
peak_values <- c(density(p1_1)$x[which.max(density(p1_1)$y)], 
                 density(p1_2)$x[which.max(density(p1_2)$y)], 
                 density(p1_3)$x[which.max(density(p1_3)$y)])
peak_densities <- c(max(density(p1_1)$y), max(density(p1_2)$y), max(density(p1_3)$y))
points(peak_values, peak_densities, col = "black", pch = 20)
text(peak_values, peak_densities, labels = round(peak_values, 2), pos = 3)
```



