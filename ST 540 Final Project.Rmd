---
title: "ST 540 Final Project"
author: "Chandler Ellsworth"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(readxl)
library(tidyverse)
library(rjags)
library(caret)
```

```{r}
#read in data
turtles <- read_excel("Turtles.xlsx") %>% 
   select(`Ending disposition`, Head, Neurologic, Respiratory, CoeolomicBreach, 
          Eye, Abscess, Shell, Extremity, Abscess2, HBC2, Infection2, OtherTrauma2, 
          UnknownFracture2, Abscess3, Infection3, Trauma3)
head(turtles)
```

# Model Fit Using dcat()
```{r}
# Set the seed for reproducibility
set.seed(123)
# Create a vector containing the row indices for the training sets
train_indices <- createDataPartition(turtles$`Ending disposition`, p = 0.8, list = FALSE)

index_count <- round(length(train_indices)/3, 0)
# Create the training sets
train_data <- turtles[train_indices, ] %>% mutate(index = sample(c(rep(1, index_count),
                                                                   rep(2,index_count),
                                                                   rep(3, index_count))))

# Create the testing sets
test_data1 <- turtles1[-train_indices1, ]
test_data2 <- turtles2[-train_indices2, ]
test_data3 <- turtles3[-train_indices3, ]

X_test1 <- test_data1 %>% select(-`Ending disposition`)
X_test2 <- test_data2 %>% select(-`Ending disposition`)
X_test3 <- test_data3 %>% select(-`Ending disposition`)

Y_test1 <- as.numeric(factor(test_data1$`Ending disposition`))
Y_test1 <- as.numeric(factor(test_data2$`Ending disposition`))
Y_test1 <- as.numeric(factor(test_data3$`Ending disposition`))

n_test1 <- length(Y_test1)
n_test2 <- length(Y_test2)
n_test3 <- length(Y_test3)

N_test1 <- length(X_test1)
N_test2 <- length(X_test2)
N_test3 <- length(X_test3)
```

# Cross Validation
```{r}
#create empty lists to store posterior means for each prediction from each model and quantiles for mod1
list1 <- list()
list2 <- list()
list3 <- list()

#create loop for cross validation where each time through the loop, the models are fit on the folds 
#not equal to the current index value and predictions are made on the fold equal to the current index value
for(i in 1:3){
  #store proper predictor data for model fitting and prediction
  X_train1_train <- train_data[train_data$index != i, ] %>% 
    select(Head, Neurologic, Respiratory, CoeolomicBreach, Eye, Abscess, Shell, Extremity)
  
  X_train1_test <- train_data[train_data$index == i, ] %>% 
    select(Head, Neurologic, Respiratory, CoeolomicBreach, Eye, Abscess, Shell, Extremity)
  
  X_train2_train <- train_data[train_data$index != i, ] %>% 
    select(Abscess2, HBC2, Infection2, OtherTrauma2, UnknownFracture2)
  
  X_train2_test <- train_data[train_data$index == i, ] %>% 
    select(Abscess2, HBC2, Infection2, OtherTrauma2, UnknownFracture2)
  
  X_train3_train <- train_data[train_data$index != i, ] %>% 
    select(Abscess3, Infection3, Trauma3)
  
  X_train3_test <- train_data[train_data$index == i, ] %>% 
    select(Abscess3, Infection3, Trauma3)

  #store proper response data for model fitting and prediction
  Y_train_train <- as.numeric(factor(train_data[train_data$index != i, 1]$`Ending disposition`))
  Y_train_test <- as.numeric(factor(train_data[train_data$index == i, 1]$`Ending disposition`))

  #store appropriate lengths and number of predictors
  N_train <- length(Y_train_train)
  N_test <- length(Y_train_test )

  n_train1 <- length(X_train1_train)
  n_train2 <- length(X_train2_train)
  n_train3 <- length(X_train3_train)
  
  #store data for JAGS
  data1   <- list(Y = Y_train_train, X1 = X_train1_train, X2 = X_train1_test, n1 = N_train, n2 = N_test, n3 = n_train1)
  #define the model
  model_string1 <- textConnection("model{
                                  
     # Likelihood
     for(i in 1:n1) {
       Y[i] ~ dcat(p[i, ])
       denom[i] <- 1 + exp(alpha1 + inprod(X1[i,], beta[])) + exp(alpha2 + inprod(X1[i,], gamma[])) 
       p[i, 1] <- 1 / denom[i]
       p[i, 2] <- exp(alpha1 + inprod(X1[i,], beta[])) / denom[i]
       p[i, 3] <- exp(alpha2 + inprod(X1[i,], gamma[])) / denom[i]
     }
     
     # Prediction
     for(i in 1:n2) {
       Y1[i] ~ dcat(p1[i, ])
       denom1[i] <- 1 + exp(alpha1 + inprod(X2[i,], beta[])) + exp(alpha2 + inprod(X2[i,], gamma[])) 
       p1[i, 1] <- 1 / denom1[i]
       p1[i, 2] <- exp(alpha1 + inprod(X2[i,], beta[])) / denom1[i]
       p1[i, 3] <- exp(alpha2 + inprod(X2[i,], gamma[])) / denom1[i]
     }
     
     # Priors
     for(j in 1:n3){
       beta[j] ~ dnorm(0, 100)
       gamma[j] ~ dnorm(0, 100)
     }
     alpha1 ~ dnorm(0, 100)
     alpha2 ~ dnorm(0, 100)
  
  }")

  #load the data and compile the MCMC code
  model1 <- jags.model(model_string1, data = data1, n.chains = 1, quiet = TRUE)
  #burn in for samples
  update(model1, 10, progress.bar = "none")
  #Generate post-burn-in samples and retain parameters of interest
  samples1 <- coda.samples(model1, variable.names = c("Y1", "beta", "gamma"), n.iter = 30, progress.bar = "none")[[1]]
  #store predictions and find category classified the most in PPD for each prediction
  preds1 <- apply(samples1[, 1:N_test], MARGIN = 2, FUN = function(x){as.numeric(names(table(x))[which.max(table(x))])})
  
  list1 <- append(list1, list(preds1))
  #store data for JAGS
  data2   <- list(Y = Y_train_train, X1 = X_train2_train, X2 = X_train2_test, n1 = N_train, n2 = N_test, n3 = n_train2)
  #define the model
  model_string2 <- textConnection("model{
                                  
     # Likelihood
     for(i in 1:n1) {
       Y[i] ~ dcat(p[i, ])
       denom[i] <- 1 + exp(alpha1 + inprod(X1[i,], beta[])) + exp(alpha2 + inprod(X1[i,], gamma[])) 
       p[i, 1] <- 1 / denom[i]
       p[i, 2] <- exp(alpha1 + inprod(X1[i,], beta[])) / denom[i]
       p[i, 3] <- exp(alpha2 + inprod(X1[i,], gamma[])) / denom[i]
     }
     
     # Prediction
     for(i in 1:n2) {
       Y1[i] ~ dcat(p1[i, ])
       denom1[i] <- 1 + exp(alpha1 + inprod(X2[i,], beta[])) + exp(alpha2 + inprod(X2[i,], gamma[])) 
       p1[i, 1] <- 1 / denom1[i]
       p1[i, 2] <- exp(alpha1 + inprod(X2[i,], beta[])) / denom1[i]
       p1[i, 3] <- exp(alpha2 + inprod(X2[i,], gamma[])) / denom1[i]
     }
     
     # Priors
     for(j in 1:n3){
       beta[j] ~ dnorm(0, 100)
       gamma[j] ~ dnorm(0, 100)
     }
     alpha1 ~ dnorm(0, 100)
     alpha2 ~ dnorm(0, 100)
  
  }")
  #load the data and compile the MCMC code
  model2 <- jags.model(model_string2, data = data2, n.chains = 1, quiet = TRUE)
  #burn in for samples
  update(model2, 10, progress.bar = "none")
  #Generate post-burn-in samples and retain parameters of interest
  samples2 <- coda.samples(model2, variable.names = c("Y1", "beta", "gamma1"), n.iter = 30, progress.bar = "none")[[1]]
  #store predictions and find category classified the most in PPD for each prediction
  preds2 <- apply(samples2[, 1:N_test], MARGIN = 2, FUN = function(x){as.numeric(names(table(x))[which.max(table(x))])})
  
  list2 <- append(list2, list(preds2))
  
  #store data for JAGS
  data3   <- list(Y = Y_train_train, X1 = X_train3_train, X2 = X_train3_test, n1 = N_train, n2 = N_test, n3 = n_train3)
  #define the model  
  model_string3 <- textConnection("model{
                                  
     # Likelihood
     for(i in 1:n1) {
       Y[i] ~ dcat(p[i, ])
       denom[i] <- 1 + exp(alpha1 + inprod(X1[i,], beta[])) + exp(alpha2 + inprod(X1[i,], gamma[])) 
       p[i, 1] <- 1 / denom[i]
       p[i, 2] <- exp(alpha1 + inprod(X1[i,], beta[])) / denom[i]
       p[i, 3] <- exp(alpha2 + inprod(X1[i,], gamma[])) / denom[i]
     }
     
     # Prediction
     for(i in 1:n2) {
       Y1[i] ~ dcat(p1[i, ])
       denom1[i] <- 1 + exp(alpha1 + inprod(X2[i,], beta[])) + exp(alpha2 + inprod(X2[i,], gamma[])) 
       p1[i, 1] <- 1 / denom1[i]
       p1[i, 2] <- exp(alpha1 + inprod(X2[i,], beta[])) / denom1[i]
       p1[i, 3] <- exp(alpha2 + inprod(X2[i,], gamma[])) / denom1[i]
     }
     
     # Priors
     for(j in 1:n3){
       beta[j] ~ dnorm(0, 100)
       gamma[j] ~ dnorm(0, 100)
     }
     alpha1 ~ dnorm(0, 100)
     alpha2 ~ dnorm(0, 100)
  
  }")
  #load the data and compile the MCMC code
  model3 <- jags.model(model_string3, data = data3, n.chains = 1, quiet = TRUE)
  #burn in for samples
  update(model3, 10, progress.bar = "none")
  #Generate post-burn-in samples and retain parameters of interest
  samples3 <- coda.samples(model3, variable.names = c("Y1", "beta", "gamma"), n.iter = 30, progress.bar = "none")[[1]]
  #store predictions and find category classified the most in PPD for each prediction
  preds3 <- apply(samples3[, 1:N_test], MARGIN = 2, FUN = function(x){as.numeric(names(table(x))[which.max(table(x))])})
  
  list3 <- append(list3, list(preds3))
}
```

# Compare model predictions with Accuracy
```{r}
#combine predictions from each fold in list into tibble
mod1_pred <- bind_cols(list1)
#create appropriate names for each column
names(mod1_pred) <- c("one", "two", "three")
#create vector of all prediction values
mod1_pred <- c(mod1_pred$one, mod1_pred$two, mod1_pred$three)
#combine predictions from each fold in list into tibble
mod2_pred <- bind_cols(list2)
#create appropriate names for each column
names(mod2_pred) <- c("one", "two", "three")
#create vector of all prediction values
mod2_pred <- c(mod2_pred$one, mod2_pred$two, mod2_pred$three)
#combine predictions from each fold in list into tibble
mod3_pred <- bind_cols(list3)
#create appropriate names for each column
names(mod3_pred) <- c("one", "two", "three")
#create vector of all prediction values
mod3_pred <- c(mod3_pred$one, mod3_pred$two, mod3_pred$three)

ordered <- train_data %>% arrange(index) %>% select(`Ending disposition`)
ordered$`Ending disposition`[ordered$`Ending disposition` == 'Death'] <- 1
ordered$`Ending disposition`[ordered$`Ending disposition` == 'Resleased'] <- 3
ordered$`Ending disposition`[ordered$`Ending disposition` == 'Euthanized'] <- 2

#create data frame of response and predicted values
preds <- data.frame(Outcome = ordered %>% unlist() %>% as.numeric(), 
                    mod1_pred, 
                    mod2_pred, 
                    mod3_pred)
#see how model compares to benchmark that classifies everything as the majority class
majority_class <- names(table(train_data$`Ending disposition`))[which.max(table(train_data$`Ending disposition`))]
majority <- ifelse(majority_class == "Death", 1, ifelse(majority_class == "Resleased", 3, 2))

data.frame(Accuracy = round(c(mean((preds$Outcome == preds$mod1_pred)), 
                   mean((preds$Outcome == preds$mod2_pred)),
                   mean((preds$Outcome == preds$mod3_pred)),
                   mean((preds$Outcome == rep(majority, length(preds$Outcome))))), 3),
           row.names = c("Model1", "Model2", "Model3","Benchmark"))
```

# Accuracy as Performance Metric
```{r}
#store predictions and find category classified the most in PPD for each prediction
preds <- apply(samples[[1]], MARGIN = 2, FUN = function(x){
  as.numeric(names(table(x))[which.max(table(x))])
})
#calculate accuracy as performance metric
(accuracy <- mean(Y == preds))
#see how model compares to benchmark that classifies everything as the majority class
majority <- as.numeric(names(table(Y))[which.max(table(Y))])
(baseline <- mean(Y == rep(majority, length(Y))))
```





# Trying to figure out workaround besides dcat or dmulti to improve computation speed
```{r}
#store data for JAGS
data <- list(Y = Y1, X = X, n = n, n_ = n_)
#define the model   
model_string <- textConnection("model{

  # Likelihood
  for(i in 1:n) {
    Y[i] ~ dbern(p1[i])
    
    p1[i] <- exp(alpha1 + inprod(X[i,],beta[])) / (1 + exp(alpha1 + inprod(X[i,],beta[])))
   
    #logit(p1[i]) <- alpha1 + inprod(X[i,],beta[])
  }
  
  # Priors
  for(j in 1:n_){
    beta[j] ~ dnorm(0,100)
    gamma[j] ~ dnorm(0,100)
  }
  alpha1 ~ dnorm(0,0.001)
  alpha2 ~ dnorm(0,0.001)
 
  }")
#load the data and compile the MCMC code
model <- jags.model(model_string, data = data, n.chains = 1, quiet = TRUE)
#burn in for samples
update(model, 5, progress.bar = "none")
#Generate post-burn-in samples and retain parameters of interest c("p1", "p2", "p3")
samples <- coda.samples(model, variable.names = c("p1"), n.iter = 10, progress.bar = "none")

samples[, 1:2]
samples[, 1924:1925]
samples[, 3847:3848]
```

```{r}
#store data for JAGS
data <- list(Y = Y2, X = X, n = n, n_ = n_)
#define the model   
model_string <- textConnection("model{

  # Likelihood
  for(i in 1:n) {
    Y[i] ~ dbern(p2[i])
    
    p2[i] <- (1 - exp(alpha1 + inprod(X[i,],beta[])) / (1 + exp(alpha1 + inprod(X[i,],beta[])))) * 
  (exp(alpha2 + inprod(X[i,],gamma[])) / (1 + exp(alpha2 + inprod(X[i,],gamma[]))))
    
    #p3[i] <- 1 - (p1[i] + p2[i])
    #logit(p2[i]) <- alpha2 + inprod(X[i,],gamma[])
  }
  
  # Priors
  for(j in 1:n_){
    beta[j] ~ dnorm(0,100)
    gamma[j] ~ dnorm(0,100)
  }
  alpha1 ~ dnorm(0,0.001)
  alpha2 ~ dnorm(0,0.001)
 
  }")
#load the data and compile the MCMC code
model <- jags.model(model_string, data = data, n.chains = 1, quiet = TRUE)
#burn in for samples
update(model, 5, progress.bar = "none")
#Generate post-burn-in samples and retain parameters of interest c("p1", "p2", "p3")
samples <- coda.samples(model, variable.names = c("p2"), n.iter = 10, progress.bar = "none")
```




