---
title: "ST 540 Final Project"
author: "Chandler Ellsworth"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(readxl)
library(tidyverse)
library(rjags)
library(caret)
```

```{r}
#read in data
turtles <- read_excel("Turtles.xlsx") %>% 
   select(`Ending disposition`, Head, Neurologic, Respiratory, CoeolomicBreach, 
          Eye, Abscess, Shell, Extremity, Abscess2, HBC2, Infection2, OtherTrauma2, 
          UnknownFracture2, Abscess3, Infection3, Trauma3)
head(turtles)
```

# Model Fit Using dcat()
```{r}
#set the seed for reproducibility
set.seed(123)
#create a vector containing the row indices for the training set
train_indices <- createDataPartition(turtles$`Ending disposition`, p = 0.8, list = FALSE)
#create number of repetitions for each number in 3 fold cross validation
index_count <- round(length(train_indices)/3, 0)
#create the training set
train_data <- turtles[train_indices, ] %>% mutate(index = sample(c(rep(1, index_count),
                                                                   rep(2,index_count),
                                                                   rep(3, index_count))))
#create test set
test_data <- turtles[-train_indices, ]
```

# Cross Validation
```{r}
#create empty lists to posterior probabilities and max classifications for each observation in test set
list1 <- list()
list2 <- list()
list3 <- list()
list5 <- list()
list6 <- list()
list7 <- list()
#create loop for cross validation where each time through the loop, the models are fit on the folds 
#not equal to the current index value and predictions are made on the fold equal to the current index value
for(i in 1:3){
  #store training predictors for model 1
  X_train1_train <- train_data[train_data$index != i, ] %>% 
    select(Head, Neurologic, Respiratory, CoeolomicBreach, Eye, Abscess, Shell, Extremity)
  #store test predictors for model 1
  X_train1_test <- train_data[train_data$index == i, ] %>% 
    select(Head, Neurologic, Respiratory, CoeolomicBreach, Eye, Abscess, Shell, Extremity)
  #store training predictors for model 2
  X_train2_train <- train_data[train_data$index != i, ] %>% 
    select(Abscess2, HBC2, Infection2, OtherTrauma2, UnknownFracture2)
  #store test predictors for model 2
  X_train2_test <- train_data[train_data$index == i, ] %>% 
    select(Abscess2, HBC2, Infection2, OtherTrauma2, UnknownFracture2)
  #store training predictors for model 3
  X_train3_train <- train_data[train_data$index != i, ] %>% 
    select(Abscess3, Infection3, Trauma3)
  #store test predictors for model 3
  X_train3_test <- train_data[train_data$index == i, ] %>% 
    select(Abscess3, Infection3, Trauma3)
  #store proper response data for model fitting and prediction
  Y_train_train <- as.numeric(factor(train_data[train_data$index != i, 1]$`Ending disposition`))
  Y_train_test <- as.numeric(factor(train_data[train_data$index == i, 1]$`Ending disposition`))
  #store appropriate lengths and number of predictors
  N_train <- length(Y_train_train)
  N_test <- length(Y_train_test )
  n_train1 <- length(X_train1_train)
  n_train2 <- length(X_train2_train)
  n_train3 <- length(X_train3_train)
  
  #store data for JAGS
  data1   <- list(Y = Y_train_train, X1 = X_train1_train, X2 = X_train1_test, n1 = N_train, n2 = N_test, n3 = n_train1)
  #define the model
  model_string1 <- textConnection("model{
                                  
     # Likelihood
     for(i in 1:n1) {
       Y[i] ~ dcat(p[i, ])
       denom[i] <- 1 + exp(alpha1 + inprod(X1[i,], beta[])) + exp(alpha2 + inprod(X1[i,], gamma[])) 
       p[i, 1] <- 1 / denom[i]
       p[i, 2] <- exp(alpha1 + inprod(X1[i,], beta[])) / denom[i]
       p[i, 3] <- exp(alpha2 + inprod(X1[i,], gamma[])) / denom[i]
     }
     
     # Prediction
     for(i in 1:n2) {
       Y1[i] ~ dcat(p1[i, ])
       denom1[i] <- 1 + exp(alpha1 + inprod(X2[i,], beta[])) + exp(alpha2 + inprod(X2[i,], gamma[])) 
       p1[i, 1] <- 1 / denom1[i]
       p1[i, 2] <- exp(alpha1 + inprod(X2[i,], beta[])) / denom1[i]
       p1[i, 3] <- exp(alpha2 + inprod(X2[i,], gamma[])) / denom1[i]
     }
     
     # Priors
     for(j in 1:n3){
       beta[j] ~ dnorm(0, 100)
       gamma[j] ~ dnorm(0, 100)
     }
     alpha1 ~ dnorm(0, 100)
     alpha2 ~ dnorm(0, 100)
  
  }")
  #load the data and compile the MCMC code
  model1 <- jags.model(model_string1, data = data1, n.chains = 1, quiet = TRUE)
  #burn in for samples
  update(model1, 5, progress.bar = "none")
  #Generate post-burn-in samples and retain parameters of interest
  samples1 <- coda.samples(model1, variable.names = c("p1", "beta", "gamma", "Y1"), n.iter = 10, progress.bar = "none")[[1]]
  #calculate PPD means for each of the 3 probabilities for each prediction
  prob_mean1 <- colMeans(samples1[, (N_test+(n_train1*2)+1): (N_test+(n_train1*2)+(N_test*3))])
  #append to list
  list1 <- append(list1, list(prob_mean1))
  #determine the category classified the most frequently for each prediction
  preds1 <- apply(samples1[, 1:N_test], MARGIN = 2, FUN = function(x){
    as.numeric(names(table(x))[which.max(table(x))])})
  #append to list
  list5 <- append(list5, list(preds1))
  
  #store data for JAGS
  data2   <- list(Y = Y_train_train, X1 = X_train2_train, X2 = X_train2_test, n1 = N_train, n2 = N_test, n3 = n_train2)
  #define the model
  model_string2 <- textConnection("model{
                                  
     # Likelihood
     for(i in 1:n1) {
       Y[i] ~ dcat(p[i, ])
       denom[i] <- 1 + exp(alpha1 + inprod(X1[i,], beta[])) + exp(alpha2 + inprod(X1[i,], gamma[])) 
       p[i, 1] <- 1 / denom[i]
       p[i, 2] <- exp(alpha1 + inprod(X1[i,], beta[])) / denom[i]
       p[i, 3] <- exp(alpha2 + inprod(X1[i,], gamma[])) / denom[i]
     }
     
     # Prediction
     for(i in 1:n2) {
       Y1[i] ~ dcat(p1[i, ])
       denom1[i] <- 1 + exp(alpha1 + inprod(X2[i,], beta[])) + exp(alpha2 + inprod(X2[i,], gamma[])) 
       p1[i, 1] <- 1 / denom1[i]
       p1[i, 2] <- exp(alpha1 + inprod(X2[i,], beta[])) / denom1[i]
       p1[i, 3] <- exp(alpha2 + inprod(X2[i,], gamma[])) / denom1[i]
     }
     
     # Priors
     for(j in 1:n3){
       beta[j] ~ dnorm(0, 100)
       gamma[j] ~ dnorm(0, 100)
     }
     alpha1 ~ dnorm(0, 100)
     alpha2 ~ dnorm(0, 100)
  
  }")
  #load the data and compile the MCMC code
  model2 <- jags.model(model_string2, data = data2, n.chains = 1, quiet = TRUE)
  #burn in for samples
  update(model2, 5, progress.bar = "none")
  #Generate post-burn-in samples and retain parameters of interest
  samples2 <- coda.samples(model2, variable.names = c("p1", "beta", "gamma", "Y1"), n.iter = 10, progress.bar = "none")[[1]]
  #calculate PPD means for each of the 3 probabilities for each prediction
  prob_mean2 <- colMeans(samples2[, (N_test+(n_train2*2)+1): (N_test+(n_train2*2)+(N_test*3))])
  #append to list
  list2 <- append(list2, list(prob_mean2))
  #determine the category classified the most frequently for each prediction
  preds2 <- apply(samples2[, 1:N_test], MARGIN = 2, FUN = function(x){
    as.numeric(names(table(x))[which.max(table(x))])})
  #append to list
  list6 <- append(list6, list(preds2))
  
  #store data for JAGS
  data3   <- list(Y = Y_train_train, X1 = X_train3_train, X2 = X_train3_test, n1 = N_train, n2 = N_test, n3 = n_train3)
  #define the model  
  model_string3 <- textConnection("model{
                                  
     # Likelihood
     for(i in 1:n1) {
       Y[i] ~ dcat(p[i, ])
       denom[i] <- 1 + exp(alpha1 + inprod(X1[i,], beta[])) + exp(alpha2 + inprod(X1[i,], gamma[])) 
       p[i, 1] <- 1 / denom[i]
       p[i, 2] <- exp(alpha1 + inprod(X1[i,], beta[])) / denom[i]
       p[i, 3] <- exp(alpha2 + inprod(X1[i,], gamma[])) / denom[i]
     }
     
     # Prediction
     for(i in 1:n2) {
       Y1[i] ~ dcat(p1[i, ])
       denom1[i] <- 1 + exp(alpha1 + inprod(X2[i,], beta[])) + exp(alpha2 + inprod(X2[i,], gamma[])) 
       p1[i, 1] <- 1 / denom1[i]
       p1[i, 2] <- exp(alpha1 + inprod(X2[i,], beta[])) / denom1[i]
       p1[i, 3] <- exp(alpha2 + inprod(X2[i,], gamma[])) / denom1[i]
     }
     
     # Priors
     for(j in 1:n3){
       beta[j] ~ dnorm(0, 100)
       gamma[j] ~ dnorm(0, 100)
     }
     alpha1 ~ dnorm(0, 100)
     alpha2 ~ dnorm(0, 100)
  
  }")
  #load the data and compile the MCMC code
  model3 <- jags.model(model_string3, data = data3, n.chains = 1, quiet = TRUE)
  #burn in for samples
  update(model3, 50, progress.bar = "none")
  #Generate post-burn-in samples and retain parameters of interest
  samples3 <- coda.samples(model3, variable.names = c("p1", "beta", "gamma", "Y1"), n.iter = 10, progress.bar = "none")[[1]]
  #calculate PPD means for each of the 3 probabilities for each prediction
  prob_mean3 <- colMeans(samples3[, (N_test+(n_train3*2)+1): (N_test+(n_train3*2)+(N_test*3))])
  #append to list
  list3 <- append(list3, list(prob_mean3))
  #determine the category classified the most frequently for each prediction
  preds3 <- apply(samples3[, 1:N_test], MARGIN = 2, FUN = function(x){
    as.numeric(names(table(x))[which.max(table(x))])})
  #append to list
  list7 <- append(list7, list(preds3))
}
```

```{r}
effectiveSize(samples1[, (N_test + 1):(N_test + (n_train1 * 2))])
effectiveSize(samples2[, (N_test + 1):(N_test + (n_train2 * 2))])
effectiveSize(samples3[, (N_test + 1):(N_test + (n_train3 * 2))])
```

```{r}
# Function to split a vector into three equal parts
split_vector <- function(vec) {
  n <- length(vec)
  n_per_col <- ceiling(n / 3)
  as.data.frame(matrix(vec, ncol = 3))
}
# Apply the function to each element of the list
df_list1 <- lapply(list1, split_vector)
df_list2 <- lapply(list2, split_vector)
df_list3 <- lapply(list3, split_vector)
# Combine the data frames by column
combined_df1 <- do.call(cbind, df_list1)
combined_df2 <- do.call(cbind, df_list2)
combined_df3 <- do.call(cbind, df_list3)
names(combined_df1) <- c("Mod1_p1", "Mod1_p2", "Mod1_p3", "Mod2_p1", "Mod2_p2", "Mod2_p3", "Mod3_p1", "Mod3_p2", "Mod3_p3")
names(combined_df2) <- c("Mod1_p1", "Mod1_p2", "Mod1_p3", "Mod2_p1", "Mod2_p2", "Mod2_p3", "Mod3_p1", "Mod3_p2", "Mod3_p3")
names(combined_df3) <- c("Mod1_p1", "Mod1_p2", "Mod1_p3", "Mod2_p1", "Mod2_p2", "Mod2_p3", "Mod3_p1", "Mod3_p2", "Mod3_p3")

preds <- rbind(combined_df1, combined_df2, combined_df3)

ordered <- train_data %>% arrange(index) %>% select(`Ending disposition`) %>% unlist() %>% as.factor() %>% as.numeric()

preds$Outcome <-ordered %>% unlist() %>% as.numeric()

values <- apply(preds, MARGIN = 1, FUN = function(x){
  if (x[10] == 1) {
    list(mod1 = x[1], mod2 = x[4], mod3 = x[7])
  } else if (x[10] == 2) {
    list(mod1 = x[2], mod2 = x[5], mod3 = x[8])
  } else {
    list(mod1 = x[3], mod2 = x[6], mod3 = x[9])
  }
})

filtered_preds <- bind_rows(values) %>% mutate(Outcome = preds$Outcome)

filtered_preds <-  filtered_preds %>% mutate(best = colnames(filtered_preds)[max.col(filtered_preds[, 1:3], "first")])

round(table(filtered_preds$best)/sum(table(filtered_preds$best)), 3)

```

```{r}
#combine predictions from each fold in list into tibble
mod1_pred <- bind_cols(list5)
#create appropriate names for each column
names(mod1_pred) <- c("one", "two", "three")
#create vector of all prediction values
mod1_pred <- c(mod1_pred$one, mod1_pred$two, mod1_pred$three)
#combine predictions from each fold in list into tibble
mod2_pred <- bind_cols(list6)
#create appropriate names for each column
names(mod2_pred) <- c("one", "two", "three")
#create vector of all prediction values
mod2_pred <- c(mod2_pred$one, mod2_pred$two, mod2_pred$three)
#combine predictions from each fold in list into tibble
mod3_pred <- bind_cols(list7)
#create appropriate names for each column
names(mod3_pred) <- c("one", "two", "three")
#create vector of all prediction values
mod3_pred <- c(mod3_pred$one, mod3_pred$two, mod3_pred$three)

#create data frame of response and predicted values
preds <- data.frame(Outcome = ordered %>% unlist() %>% as.numeric(), 
                    mod1_pred, 
                    mod2_pred, 
                    mod3_pred)
#see how model compares to benchmark that classifies everything as the majority class
majority_class <- names(table(train_data$`Ending disposition`))[which.max(table(train_data$`Ending disposition`))]
majority <- ifelse(majority_class == "Death", 1, ifelse(majority_class == "Released", 3, 2))

data.frame(Accuracy = round(c(mean((preds$Outcome == preds$mod1_pred)), 
                   mean((preds$Outcome == preds$mod2_pred)),
                   mean((preds$Outcome == preds$mod3_pred)),
                   mean((preds$Outcome == rep(majority, length(preds$Outcome))))), 5),
           row.names = c("Model1", "Model2", "Model3", "Benchmark"))
```

# Accuracy as Performance Metric
```{r}
#store predictions and find category classified the most in PPD for each prediction
#preds <- apply(samples[[1]], MARGIN = 2, FUN = function(x){
 # as.numeric(names(table(x))[which.max(table(x))])
#})
#calculate accuracy as performance metric
#(accuracy <- mean(Y == preds))
#see how model compares to benchmark that classifies everything as the majority class
#majority <- as.numeric(names(table(Y))[which.max(table(Y))])
#(baseline <- mean(Y == rep(majority, length(Y))))
```

# Trying to figure out workaround besides dcat or dmulti to improve computation speed
```{r}
#store data for JAGS
data <- list(Y = Y1, X = X, n = n, n_ = n_)
#define the model   
model_string <- textConnection("model{

  # Likelihood
  for(i in 1:n) {
    Y[i] ~ dbern(p1[i])
    
    p1[i] <- exp(alpha1 + inprod(X[i,],beta[])) / (1 + exp(alpha1 + inprod(X[i,],beta[])))
   
    #logit(p1[i]) <- alpha1 + inprod(X[i,],beta[])
  }
  
  # Priors
  for(j in 1:n_){
    beta[j] ~ dnorm(0,100)
    gamma[j] ~ dnorm(0,100)
  }
  alpha1 ~ dnorm(0,0.001)
  alpha2 ~ dnorm(0,0.001)
 
  }")
#load the data and compile the MCMC code
model <- jags.model(model_string, data = data, n.chains = 1, quiet = TRUE)
#burn in for samples
update(model, 5, progress.bar = "none")
#Generate post-burn-in samples and retain parameters of interest c("p1", "p2", "p3")
samples <- coda.samples(model, variable.names = c("p1"), n.iter = 10, progress.bar = "none")

samples[, 1:2]
samples[, 1924:1925]
samples[, 3847:3848]
```

```{r}
#store data for JAGS
data <- list(Y = Y2, X = X, n = n, n_ = n_)
#define the model   
model_string <- textConnection("model{

  # Likelihood
  for(i in 1:n) {
    Y[i] ~ dbern(p2[i])
    
    p2[i] <- (1 - exp(alpha1 + inprod(X[i,],beta[])) / (1 + exp(alpha1 + inprod(X[i,],beta[])))) * 
  (exp(alpha2 + inprod(X[i,],gamma[])) / (1 + exp(alpha2 + inprod(X[i,],gamma[]))))
    
    #p3[i] <- 1 - (p1[i] + p2[i])
    #logit(p2[i]) <- alpha2 + inprod(X[i,],gamma[])
  }
  
  # Priors
  for(j in 1:n_){
    beta[j] ~ dnorm(0,100)
    gamma[j] ~ dnorm(0,100)
  }
  alpha1 ~ dnorm(0,0.001)
  alpha2 ~ dnorm(0,0.001)
 
  }")
#load the data and compile the MCMC code
model <- jags.model(model_string, data = data, n.chains = 1, quiet = TRUE)
#burn in for samples
update(model, 5, progress.bar = "none")
#Generate post-burn-in samples and retain parameters of interest c("p1", "p2", "p3")
samples <- coda.samples(model, variable.names = c("p2"), n.iter = 10, progress.bar = "none")
```

```{r}
  #store data for JAGS
  data4   <- list(Y = as.numeric(factor(train_data$`Ending disposition`)), 
                  X1 = train_data %>% select(Head, Neurologic, Respiratory, CoeolomicBreach, Eye, Abscess, Shell, Extremity), 
                  n1 = length(as.numeric(factor(train_data$`Ending disposition`))), 
                  n3 = 8)
  #define the model
  model_string4 <- textConnection("model{
                                  
     # Likelihood
     for(i in 1:n1) {
       Y[i] ~ dcat(p[i, ])
       denom[i] <- 1 + exp(alpha1 + inprod(X1[i,], beta[])) + exp(alpha2 + inprod(X1[i,], gamma[])) 
       p[i, 1] <- 1 / denom[i]
       p[i, 2] <- exp(alpha1 + inprod(X1[i,], beta[])) / denom[i]
       p[i, 3] <- exp(alpha2 + inprod(X1[i,], gamma[])) / denom[i]
     }
     
     # Priors
     for(j in 1:n3){
       beta[j] ~ dnorm(0, 100)
       gamma[j] ~ dnorm(0, 100)
     }
     alpha1 ~ dnorm(0, 100)
     alpha2 ~ dnorm(0, 100)
  
  }")

  #load the data and compile the MCMC code
  model4 <- jags.model(model_string4, data = data4, n.chains = 1, quiet = TRUE)
  #burn in for samples
  update(model4, 5, progress.bar = "none")
  #Generate post-burn-in samples and retain parameters of interest
  samples4 <- coda.samples(model4, variable.names = c("p", "beta", "gamma"), n.iter = 10, progress.bar = "none")[[1]]
```



